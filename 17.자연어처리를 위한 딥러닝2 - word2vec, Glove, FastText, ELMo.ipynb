{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "[실습]45~46.자연어처리를 위한 딥러닝2 - word2vec, Glove, FastText, ELMo.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U5F3fcRXYcL"
      },
      "source": [
        "# Word Embedding\n",
        "\n",
        "> the collective name for a set of language modeling and feature learning techniques in NLP where words and phrases from the vocabulary are mapped to vectors of real numbers. (from Wikipedia)\n",
        "\n",
        "> 수학적으로, 고차원의 공간을 더 낮은 공간으로 변환하는 방법(embedding)과 같은 의미이기도 하다.\n",
        "\n",
        "> 결국, 고차원으로 표현된 feature vector(local representation, BOW, TF-IDF 등)을 distributional semantic을 가지는 vector space에 mapping 시켜주는 방법이다.\n",
        "\n",
        "> <b>\"You shall know a word by the company it keeps\"(John R. Firth, 1957)<b>, it called \"Distributed Hypothesis\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cacLlelRXYcM"
      },
      "source": [
        "![word embedding fig](figs/word-vector-space-similar-words.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PcG4T3xXYcM"
      },
      "source": [
        "![visualize word vectors](figs/visualize-word-vectors.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNA1IFNBXYcN"
      },
      "source": [
        "다음은 최근 많이 쓰이는 word embedding 방법들이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1gjy3txXYcN"
      },
      "source": [
        "wevi : word embedding visual inspector\n",
        "    \n",
        "> https://ronxin.github.io/wevi/\n",
        "\n",
        "이론을 정리하기 위해 체험을 해보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOsnirRXXYcN"
      },
      "source": [
        "## 1. Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j876ISeXYcO"
      },
      "source": [
        "> 현재 word embedding이 핫하게 된 시작 알고리즘. \"Distributed representations of words and phrases and their compositionality(NIPS 2013)\" 에 처음 소개되었다.\n",
        "\n",
        "> Reference : https://code.google.com/archive/p/word2vec/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjJXx-RUXYcO"
      },
      "source": [
        "![skip-gram](figs/skip-gram.png)\n",
        "![simple-skip-gram](figs/simple-skip-gram.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iec1ucVxXYcO"
      },
      "source": [
        "> Skip-Gram with Negative Sampling, 줄여서 SGNS라고 부르며 Neural Net을 이용한 word embedding이 빠르게 구현가능해진 이유기도 하다.\n",
        "\n",
        "> Negative Sampling이란, 마지막 단계의 softmax를 구하는 문제를 주변 단어(postive class)와 무작위로 골라진 나머지 단어들(negative class)로 분류하는 binary classfication 문제로 바꿔주는 기법이며 이를 통해 굉장히 빠르게 word embedding 수행이 가능하다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwWrS41rXYcO",
        "outputId": "f858a490-43dc-4eea-e694-33e52fd79559"
      },
      "source": [
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_fVJ8PqXYcP",
        "outputId": "2eb3b5eb-f3b4-4009-aab9-f4be7e05d890"
      },
      "source": [
        "from nltk.corpus import movie_reviews\n",
        "sentences = [list(sent) for sent in movie_reviews.sents()]\n",
        "sentences[0]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['plot',\n",
              " ':',\n",
              " 'two',\n",
              " 'teen',\n",
              " 'couples',\n",
              " 'go',\n",
              " 'to',\n",
              " 'a',\n",
              " 'church',\n",
              " 'party',\n",
              " ',',\n",
              " 'drink',\n",
              " 'and',\n",
              " 'then',\n",
              " 'drive',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoR1KQZWXYcR"
      },
      "source": [
        "# gensim에서 word2vec 불러오기\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# 학습 모델 구현.\n",
        "w2v_model = Word2Vec(sentences, min_count=5, size=300, sg=1, iter=10, workers=4, ns_exponent=0.75, window=7)  # sg=1 => Skip Gram을 사용하겠다. / workers=4 => CPU를 4개 정도 사용하겠다."
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ch05nRnYXYcR",
        "outputId": "37130e36-95a3-4b03-9f40-0c68fa190289"
      },
      "source": [
        "# 모델 평가, 비슷한 단어 찾기.\n",
        "w2v_model.wv.most_similar(\"good\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('decent', 0.5294654369354248),\n",
              " ('darned', 0.48018771409988403),\n",
              " ('meaty', 0.4768997132778168),\n",
              " ('lousy', 0.4737260937690735),\n",
              " ('dopey', 0.47170382738113403),\n",
              " ('milestone', 0.4691770374774933),\n",
              " ('gutsy', 0.466529905796051),\n",
              " ('fantastic', 0.4641210436820984),\n",
              " ('commendable', 0.44876447319984436),\n",
              " ('crappy', 0.4473940134048462)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyu_4Ac6XYcR"
      },
      "source": [
        "## 2. GloVe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wGSVt7kXYcR"
      },
      "source": [
        "> GloVe는 Gloval Vectors의 약자로, aggregated global word co-occurence statistics를 최적화하는 방향으로 학습하는 word embedding 방법이다. \"GloVe: Gloval Vectors for Word Representation(EMNLP 2014)\"에 소개되었다.\n",
        "\n",
        "> Reference : https://nlp.stanford.edu/projects/glove/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFUZMY38XYcR"
      },
      "source": [
        "![glove](figs/glove.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrYrMAapXYcR"
      },
      "source": [
        "# glove는 현재 파이썬으로는 구현이 힘듭니다, 사용을 위해서는 c++을 사용하셔야 합니다.\n",
        "#from glove import Glove\n",
        "\n",
        "#glove_model = Glove(no_components=100, learning_rate=0.05)\n",
        "#glove_model.fit(sentences, epochs=10, no_threads=4, verbose=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UHj3ttEXYcR"
      },
      "source": [
        "## 3. FastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO_jAVhUXYcS"
      },
      "source": [
        "> 현재 NLP task에서 word embedding의 baseline으로 사용되는 기법이다. subword embedding model, char n-gram embedding model이라고도 한다.\n",
        "\n",
        "> word2vec을 만들었던, Tomas Mikolov가 Google에서 Facebook으로 옮긴 뒤에 낸 모델로 word2vec의 단점을 보완한 모델이다.\n",
        "\n",
        "> word2vec의 단점이었던, OOV 문제와 low frequency를 많이 해결하였다.\n",
        "\n",
        "> word를 subword 단위로 표현하는 것으로 기본적으로 SGNS 방식이다.\n",
        "\n",
        "> Reference : https://fasttext.cc/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypCoFx-rXYcS"
      },
      "source": [
        "![char3-grams](figs/char3-grams.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-Aul7yXXYcS"
      },
      "source": [
        "# gensim에서 FastText 불러오기\n",
        "from gensim.models import FastText\n",
        "\n",
        "# FastText 학습.\n",
        "fast_model = FastText(sentences, min_count=5, sg=1, size=300, workers=4, min_n=2, max_n=7, alpha=0.05, iter=10, window=7)\n",
        "fast_model.save(\"fast_model\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHafBtlbXYcS",
        "outputId": "629c56c7-3104-48da-d06b-4de663bdeb8b"
      },
      "source": [
        "# 결과 평가.\n",
        "fast_model.wv.most_similar(\"good\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('goods', 0.49452564120292664),\n",
              " ('goodnight', 0.41828882694244385),\n",
              " ('goofball', 0.4050053358078003),\n",
              " ('goose', 0.391421914100647),\n",
              " ('great', 0.3896257281303406),\n",
              " ('goo', 0.3877134919166565),\n",
              " ('bad', 0.3793599307537079),\n",
              " ('goop', 0.36883866786956787),\n",
              " ('decent', 0.36726880073547363),\n",
              " ('excellent', 0.365870863199234)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdKZfHtHXYcS"
      },
      "source": [
        "## 4. ELMo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e17g491UXYcS"
      },
      "source": [
        "![elmo_sesame_street](figs/elmo_sesame_street.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZorTbBvuXYcS"
      },
      "source": [
        "> ELMo는 Embeddings from Language Model의 약자입니다. ELMo는 pre-trained language model을 사용하여 문맥에 맞는 word embedding, \"Contextualized Word Embedding\"을 만드는 방법입니다.\n",
        "\n",
        "> bidirectional Language Model을 이용하여, pre-trained embedding vector를 corpus의 context(syntax, semantics, polysemy) 정보를 보완해주는 embedding vector를 만들어 준다.\n",
        "\n",
        "> tensorflow, pytorch를 통해서 bidirectional LSTM model을 만들어 사용이 가능하다. (이미 구현된 model이 github에 공개되어있다.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpFE4SugXYcS"
      },
      "source": [
        "> \"Deep contextualized word representations(NAACL 2018)\"에 소개된 방법입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IReoBLKhjOl"
      },
      "source": [
        "#!pip install tensorflow_hub"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdOURzaUXYcT"
      },
      "source": [
        "# # tensorflow_hub에서 데이터 불러오기.\n",
        "# import tensorflow_hub as hub\n",
        "\n",
        "# # 가까운 미래에는 실행이 되길 빌면서..\n",
        "# elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")\n",
        "\n",
        "# embeddings = elmo(\n",
        "#     [\"the cat is on the mat\", \"dogs are in the fog\"],\n",
        "#     signature=\"default\",\n",
        "#     as_dict=True)[\"elmo\"]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF_1afXrXYcT"
      },
      "source": [
        "# print(embeddings)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLAYc2a5XYcT"
      },
      "source": [
        "> Reference : https://allennlp.org/elmo, https://github.com/allenai/bilm-tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zja7UtZpXYcT"
      },
      "source": [
        "![elmo_architecture](figs/elmo_structure.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtYCadkmXYcT"
      },
      "source": [
        "![elmo_model](figs/elmo_model.png)"
      ]
    }
  ]
}